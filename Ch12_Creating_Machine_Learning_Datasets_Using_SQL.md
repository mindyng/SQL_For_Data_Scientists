A binary classification model predicts whether a record belongs to one category or another. For example, a heart disease classification model might analyze data from a patient's medical history to determine whether they're likely to develop heart disease, or not. A weather model could use past and current temperature, precipitation, pressure, and wind measurements, as well as those from surrounding geographic areas, to predict whether or not it will rain in the next 24 hours. In a retail scenario like a farmer's market, the seller may want to predict whether a customer will return to make another purchase within a certain time frame, or not.

In order to make predictions, the model needs to be trained. Binary classifiers are a type of supervised learning model, which means they are trained by passing example rows of data (also called instances, observations, or feature vectors) labeled with each of the possible outcomes into the algorithm, so it can detect patterns and identify characteristics that are more strongly associated with one result or the other. Some example instances are set aside to test the trained model, feeding them into the algorithm to generate predictions we can compare to the known actual outcomes in order to check the model's performance and determine in what ways the model is incorrect, so we can make adjustments to it.

A time series model performs statistical operations on a series of measurements over time to forecast what the measurement might be at some point in time in the future. The training data for a time series model is a running log of data measurements from past points in time. Someone could use an hourly history of a stock's prices to attempt to predict the value of an investment at the end of the day. A college might use historical counts of applications, admission offers, enrolled students, and deposits paid per week to generate a weekly forecast of incoming freshman class enrollment. A farmer's market may use purchases over time to detect seasonal product sales trends and growth in the customer base, or try to predict how many ears of corn will sell per week next month.

Each type of model requires a different type of dataset, and we'll review some approaches for preparing datasets for these two common types of models using SQL.

Let's create a dataset that allows us to plot a time series of farmer's market sales per week. Note that this will be a simplistic view of sales, because it will not take into consideration changes in vendors over time, available inventory at different times of year, or external economic factors.

There is no information in our dataset indicating that each week in our dataset actually represents sales from two market dates (as opposed to, say, seven different days of sales summarized by week). And if we asked Tableau to forecast monthly sales using this dataset, it only has the date of the first market per week, so if a new month started between two market dates we grouped into a single week, the second market's sales will be categorized into the wrong month, creating erroneous training data for the forecasting algorithm.

It's important to know the intended use of a dataset in order to make the correct choices when designing it. It's also important to provide documentation to accompany a dataset, so others who might use it later are aware of its original purpose, and any caveats that might be important to know when using it for analysis.

Datasets for Binary Classification
Classification algorithms detect patterns in training datasets, which contain example records from the past that fall into known categories, and use those detected patterns to then categorize new data. Binary classification algorithms categorize inputs into one of two outcomes, which are determined according to the purpose of the model and the available data. For example: Given the medical history of a patient, is it likely that they have heart disease, or not? Given the current and summarized past weather data, is it likely to rain within the next 24 hours, or not? Many of these algorithms can also output a probability or likelihood score, so you can also determine how “sure” the algorithm is that the instance should be classified into one category or the other (how well it matches the patterns detected for training examples in either class).

These algorithms need training data that is in the same form as the data to be classified. For example, if you want the algorithm to take a patient's medical history, including current vital measurements, as input, then the training data has to be at the same granularity and level of summary. Based on the dataset it was trained on, your classification model may expect one row of data per patient, with input fields such as a patient's age, sex, cholesterol as of five years ago, cholesterol as of one year ago, cholesterol measured today (the day of diagnosis), number of years that the patient has smoked cigarettes (as of the day of diagnosis), resting blood pressure as of five years ago, resting blood pressure as of one year ago, resting blood pressure measured today, resting ECG results, chest pain level indicator, and other summary metrics. In this case, each training “instance” (row of data, or vector) should have the data as of the diagnosis date, as well as the measurements or cholesterol and blood pressure from one and five years prior to the diagnosis date, so the duration between those two data points in the training data is as similar as possible to the duration between those two data points in the data you are passing through the trained model to be classified. The conditions under which the model will be applied need to be considered when designing the dataset.

We won't get into the details of how much past data with known outcomes is required for training and testing a model here, as it depends on the type of model, the number of columns, the variation in the data values, and many other factors beyond the scope of this book. We also won't be training classification models in this book. However, we will discuss how to structure the datasets needed for binary classification model training and prediction, and how to use SQL to pull the data you need to train and run a classifier.

When thinking about how to structure a dataset for binary classification, the first thing you'll need to determine is the target variable, meaning the categories you're building a model to classify records into. Often, the target variable needs a time bounding, so instead of predicting “Will this patient develop heart disease?” the outcome being predicted could be “Will this patient be diagnosed with heart disease in the next five years?” The time bounding will affect choices you make when designing the training dataset.

The benefit of having multiple records per customer is that a lot more training instances are available for the model to use to detect patterns in the data. Additionally, a person's behavior can change over time, so having a snapshot record of their summary activity every time they make a purchase, and a record of whether they came back within a month of making that purchase, can help the algorithm determine what impact certain activities may have on behavior. One effect of this approach to be aware of is that frequent customers will be over-represented in the dataset, which could have different impacts depending on the model, and could lead to overfitting the model to that type of customer. (Because a one-time customer will only be in the training dataset one time, while a frequent customer will be in the training dataset many times.)

Setting up your query to produce a dataset that is at the correct granularity with a target variable that is time-bound can be the most complicated step of the query design process, and it's worth taking the time to make sure it is calculated correctly before pulling in any other data fields. 

The purchase_total column should be familiar by now, multiplying the quantity and cost of each item purchased and summing that up to get the total spent by each customer at each market date. The vendors_patronized column is a distinct count of how many different vendors the customer made purchases from that day, and the different_products_purchased column is a distinct count of how many different kinds of products the customer purchased. These calculated columns are also called engineered features when you're talking about datasets for machine learning, and we're including them so we can explore the relationship between these values and the target variable. Maybe the more vendors the customer makes purchases from, the more likely they are to purchase an item they like so much that they'll return within 30 days to buy more. Including this column in the dataset enables the exploration of the relationship between this feature and the target variable.

The customer_next_market_date and days_until_customer_next_market are useful for validating the output of the preceding query, but once we have checked that the target variable purchased_again_within_30_days looks correct in context, we can remove those two columns from our machine learning dataset. All of the columns other than the target variable will be available as inputs to our algorithm, so we don't want to encode that value indicating whether or not they returned within 30 days in any other field, to avoid data leakage. Data leakage occurs when data that would not have been known as of the snapshot in time that the row represents is fed into the algorithm, artificially improving the predictions.

Another type of aggregate value that might be useful to input into a predictive model is some type of representation of the customer's entire history of farmer's market shopping, up to the date the row represents. For example, how many times has the customer shopped at the farmer's market before? A long-time shopper might be more likely to return than a brand-new shopper.

The ROW_NUMBER window function is one way to calculate this value, counting how many prior rows exist for each customer, but you have to be careful where you put it in the query, because ROW_NUMBER only counts the rows returned by the query. So, if we wanted to count how many times the customer has shopped at the market before as of the market date in the current row, but our main query is filtered to only return data from the year 2019, then our ROW_NUMBER window function will only be able to count previous purchases in 2019 and not the customer's entire shopping history.

he reason the ROW_NUMBER returns much higher counts when the query is summarized using COUNT DISTINCT is that the window function is calculated on the dataset before the DISTINCT , so since the results aren't grouped, it's returning one row per customer per product purchased, not one row per customer per market_date , then numbering every row in the customer_purchases table for that customer. Grouping by market_date solves that issue because the window function is calculated after the data is aggregated by the GROUP BY . Sometimes it takes trial and error to get the order of operations correct, and this is why it's important to view the details of the underlying data prior to aggregating, so you know whether the resulting summary data is correct.

One important factor when engineering features is that each of these feature values is only what would be knowable as of the date represented by the row—the market_date in this case. We want to train the model on examples of customers with a variety of traits as of specific points in time that can be correlated with a specific outcome or target variable relative to that time. In fact, I've been outputting the market_date in each of the previous queries for verification purposes, but I wouldn't input the full date into a predictive model, because then the training data would all be tied to past dates, when only the relative dates for the events of interest (the time between purchases) are important. If we ran a current customer's record through the model to try to make a prediction based on data collected this week, but the model had been trained on full date values from the past, the model wouldn't know what to do with the market_date , because there will have been no training examples with similar dates. So, when I train my classification model using this dataset, I will not input the customer_id or market_date into the algorithm. I will only use them as unique identifiers, or index values, so the predictions the model outputs can be tied back to their respective rows.

Sometimes you will be joining in data from additional tables to add columns to your dataset, and you'll have to be careful not to change the granularity as you do so. In this case, we engineered features using only data in the customer_purchases table in the Farmer's Market database, aggregating data from the same table in many ways. You can use the SQL you have learned in this book to engineer a wide variety of features, improving your model by providing it with many different signals to correlate with the target variable.

Benefits of conducting some feature engineering in the SQL code as a separate step in your data pipeline include the ability to easily store your results in a database table to use repeatedly during training (without having to regenerate the calculated columns each time your script is run) or to share with others. Additionally, some types of summarization are more efficient to do in SQL at the point of data extraction from the database than in other coding environments. If you need it to run more quickly, you could ask an experienced data engineer to help make your SQL more computationally efficient, once you have it returning the results you want. Now that you know how to build your own dataset, you can provide them with a query that generates the results you need instead of having to explain the granularity and define every column. 
